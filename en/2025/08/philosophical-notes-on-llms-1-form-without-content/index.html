<!DOCTYPE html>
<html lang="en">
<head>
    <meta http-equiv="content-type" content="text/html; charset=UTF-8">
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="generator" content="Eleventy v3.0.0">
    <title>Philosophical notes on LLMs 1: form without content – La màquina de Turing</title>
    <meta property="og:title" content="Philosophical notes on LLMs 1: form without content">
    <meta property="og:type" content="article">
    <meta property="og:image" content="https://lamaquinadeturing.su/assets/media/tinman_opt1200.jpg">
    <meta property="og:url" content="https://lamaquinadeturing.su/en/2025/08/philosophical-notes-on-llms-1-form-without-content/">
    <style>/* assets/css/inlined.css */
html {
    font-size: 20px;
    --font-title: "Pixelar", -apple-system, BlinkMacSystemFont, sans-serif;
    --font-body: "Barlow", -apple-system, BlinkMacSystemFont, sans-serif;
    --font-size-18: .9rem;
    --color-text: hsl(0, 0%, 0%);
    --color-indifferent: hsl(0, 0%, 40%);
    --color-keyword: hsl(0, 0%, 60%);
    --color-accent: hsl(120, 95%, 42%);
    --color-title: hsl(285, 4%, 21%);
    --color-border: hsl(285, 4%, 21%);
    --color-bg: hsl(0, 0%, 100%);
}

*,
*::before,
*::after {
    box-sizing: border-box;
}

body {
    margin: 0;
    font-size: var(--font-size-18);
    font-family: var(--font-body);
    line-height: 1.8;
    color: var(--color-text);
    overflow-x: hidden;
    font-weight: normal;
    background: url("/assets/img/dots.svg") var(--color-bg);
    background-size: 2px 2px;
    background-attachment: fixed;
}

h1, h2, h3, h4, h5, h6 {
    color: var(--color-title);
    margin: 1em 0 .5em 0;
    line-height: 1.4;
    font-weight: normal;
}

h1 { font-size: 2.4rem; }
h2 { font-size: 2rem; }
h3 { font-size: 1.7rem; }
h4 { font-size: 1.5rem; }
h5 { font-size: 1.3rem; }
h6 { font-size: 1.1rem; }

article h1 { font-size: 1.7rem; }
article h2 { font-size: 1.5rem; }
article h3 { font-size: 1.3rem; }
article h4 { font-size: 1.1rem; }
article h5 { font-size: 0.9rem; }
article h6 { font-size: 0.7rem; }
</style>
<link rel="stylesheet" href="/assets/css/linked.css" type="text/css">
<script>
function loadStyleSheets(...sources) {
    for (const src of sources) {
        if (document.createStyleSheet) document.createStyleSheet(src);
        else {
            var stylesheet = document.createElement('link');
            stylesheet.href = src;
            stylesheet.rel = 'stylesheet';
            stylesheet.type = 'text/css';
            document.getElementsByTagName('head')[0].appendChild(stylesheet);
        }
    }
}
loadStyleSheets('/assets/css/deferred.css');
</script>
<script src="/assets/js/combined.js" defer></script>
    <link rel="alternate" type="application/atom+xml" href="https://lamaquinadeturing.su/feed.xml" title="La màquina de Turing">
    <link rel="icon" type="image/png" sizes="32x32" href="/favicon-32x32.png">
    <link rel="icon" type="image/png" sizes="16x16" href="/favicon-16x16.png">
    <link rel="dns-prefetch" href="https://fonts.googleapis.com/">
    <link rel="dns-prefetch" href="https://s.w.org/">
    <script>
        const page_lang = "en";
        const site_languages = ["ca","en","es"];
    </script>
    <!-- Matomo -->
    <script>
        var _paq = window._paq = window._paq || [];
        /* tracker methods like "setCustomDimension" should be called before "trackPageView" */
        _paq.push(['trackPageView']);
        _paq.push(['enableLinkTracking']);
        (function() {
        var u="//stats.factoria.lu/";
        _paq.push(['setTrackerUrl', u+'matomo.php']);
        _paq.push(['setSiteId', '3']);
        var d=document, g=d.createElement('script'), s=d.getElementsByTagName('script')[0];
        g.async=true; g.src=u+'matomo.js'; s.parentNode.insertBefore(g,s);
        })();
    </script>
    <!-- End Matomo Code -->
</head>
<body class="home blog eightbit-effect loading" itemscope="" itemtype="http://schema.org/WebPage">
    <div id="page">

        <header id="site-header" itemscope="" itemtype="http://schema.org/WPHeader">
    <div id="branding-wrapper" class="element__frame">
        <a href="/en" rel="home" itemscope="" itemtype="http://schema.org/Brand">
            <h1 class="site-title" itemprop="name">La màquina de Turing</h1>
        </a>
    </div>

    <nav class="element__frame desktop-media">
        <!-- <p class="bar__title">Lang</p> -->

        <ul class="pull clear">
            <li class="menu-item"><span>en</span></li>
            
                
                    <li class="menu-item"><a href="/ca">ca</a></li>
                
            
                
            
                
                    <li class="menu-item"><a href="/es">es</a></li>
                
            
        </ul>
    </nav>

    <nav itemscope="" itemtype="http://schema.org/SiteNavigationElement" class="element__frame" id="mobile-window">
        <p class="bar__title mobile-media">Menu</p>

        <a id="nav-toggle" class="nav-slide-button" href="#"><span></span></a>

        <ul id="primary-menu" class="pull clear">
            <li class="menu-item mobile-media"><span>en</span></li>
            
                
                    <li class="menu-item mobile-media"><a href="/ca">ca</a></li>
                
            
                
            
                
                    <li class="menu-item mobile-media"><a href="/es">es</a></li>
                
            

            <li>
                <div class="search-form">
                    <label>
                        <span class="screen-reader-text">Search for:</span>
                        <input type="search" class="search-field" placeholder="Search…" name="q">
                    </label>
                    <input type="submit" class="search-submit" value="Search">
                </div>
            </li>

            
            
            <li class="menu-item desktop-media"><a href="/en/cybernetics/">Cybernetics</a></li>
            
            <li class="menu-item desktop-media"><a href="/en/lzhenauka/">Lzhenauka</a></li>
            
            <li class="menu-item desktop-media"><a href="/en/web/">Web</a></li>
            
        </ul>

        <ul class="search-results hidden"></ul>
    </nav>

    
    <ul class="social-media-profiles element__frame">
        <li>
            <p class="bar__title">
                Social </p>
        </li>
        
            
            
            
        <li><a href="https://mastodont.cat/@urixturing"><img src="/assets/img/mastodon.svg" width="12" height="12"></a></li>
        
        
        <li><a href="https://github.com/maquinadeturing/lamaquinadeturing.su"><img src="/assets/img/github.svg" width="12" height="12"></a></li>
        
        
        <li><a href="/feed.xml"><img src="/assets/img/rss.svg" width="12" height="12"></a></li>
        
    </ul>
    
</header>
<div class="clear"></div>

        
        <section class="grid__single" itemscope="" itemtype="http://schema.org/BlogPost">

            
<div class="grid__item">
    <article itemprop="blogPost" itemscope="" itemtype="http://schema.org/BlogPosting" id="post-5"
        class="element__frame post type-post status-publish format-standard has-post-thumbnail hentry">
        <header class="entry-header">
            <span class="posted-on">22 Aug 2025</span>
            
            <div>
            
            
            <a href="/en/cybernetics/" class="category-name" itemprop="articleSection">Cybernetics</a>
            
            </div>
            
            <h2 class="entry__title" itemprop="headline">Philosophical notes on LLMs 1: form without content</h2>
        </header>
        <div class="entry-content" itemprop="articleBody">
            
            
            
            
            <div class="entry-thumbnail-single landscape" itemprop="image">
                <img src="/assets/media/tinman_opt.jpg" srcset="/assets/media/tinman_opt698.jpg 698w" sizes="(max-width: 800px) 698px, (max-width: 1200px) 1078px, 698px" class="attachment-large size-large wp-post-image" alt="The Tin Woodman with Alice and the Scarecrow, in W. W. Denslow’s Illustrations for the Wonderful Wizard of Oz (1900).">
            </div>
            
            <div class="entry-thumbnail-caption"><p>The Tin Woodman with Alice and the Scarecrow, in <a href="https://en.wikipedia.org/wiki/W._W._Denslow" class="link-wikipedia">W. W. Denslow’s</a> <em>Illustrations for the Wonderful Wizard of Oz</em> (1900).</p>
</div>
            
            

            
            
                <p class="entry__meta translations-box"><span class="lang-icon"></span> Also available in:
                    
                        <a href="/ca/2025/08/notes-filosofiques-sobre-la-ia-1-forma-sense-contingut/" class="box-shadow lang-code">ca</a>
                    
                </p>
            

            
<p><em>What follows are some of notes trying to understand large language models with the help of different philosophical theories. I'm incredibly out of my element, so treat this text as experimental.</em></p>
<p><em>As the title implies, more may follow.</em></p>
<div class="toc"><ul><li><a href="#large-language-models-are-completion-models">Large language models are completion models </a></li><li><a href="#form-without-content%3A-searle">Form without content: Searle </a></li><li><a href="#empty-words%3A-plato%2C-aristotle">Empty words: Plato, Aristotle </a></li><li><a href="#chatter%3A-kierkegaard%2C-heidegger">Chatter: Kierkegaard, Heidegger </a></li><li><a href="#notes">Notes </a></li></ul></div>
<h2 id="large-language-models-are-completion-models" tabindex="-1">Large language models are completion models <a class="heading-link" href="#large-language-models-are-completion-models">§</a></h2>
<p>When a user writes a prompt, the chat bot responds with a reply. That is the foundation of the modern generative AI as most people know it. This interaction is useful because you can ask a question and get a response, or describe a task and get the result.</p>
<p>This gives the impression of intelligence, as the bot can do really complex tasks. From summarizing large texts at incredible speed, to maintaining an interesting conversation, programming an algorithm, or solving a difficult math problem. How did the companies that make these bots train them to do all of this?</p>
<p>The answer is not an incredibly dense program, but a clever trick: the bot just learned what is the most likely word to follow a sentence.</p>
<p>It really is more complex than this<sup class="footnote-ref"><a href="#fn1" id="fnref1">[1]</a></sup><sup class="footnote-ref"><a href="#fn2" id="fnref2">[2]</a></sup>, but that's the essence. The bot looks at the prompt from the user, then adds the first word of the reply. Then another, and another. This is why they are also called <em>completion</em> models<sup class="footnote-ref"><a href="#fn3" id="fnref3">[3]</a></sup>.</p>
<p>In their core, chat bots only really know how to add one word. But they are <em>really good</em> at it. Their whole world is adding just one word, the perfect one. After that, they do it again. This may generate several questions. How is it possible that coherent sentences, or even insightful ideas, are created this way, instead of nonsense? Well, the same way the game of <a href="https://en.wikipedia.org/wiki/Consequences_(game)" class="link-wikipedia">Consequences</a> can create an <a href="https://en.wikipedia.org/wiki/Exquisite_corpse" class="link-wikipedia">exquisite corpse</a>, one word at a time.</p>
<h2 id="form-without-content%3A-searle" tabindex="-1">Form without content: Searle <a class="heading-link" href="#form-without-content%3A-searle">§</a></h2>
<p>Do large language models <em>know</em> what they say? Do <em>we</em> know what we say?</p>
<p>The most direct reference to chat bots is <strong>John Searl's</strong> <a href="https://en.wikipedia.org/wiki/Chinese_room" class="link-wikipedia">Chinese room</a> argument. It goes like this: your job is to stay in a room, with a big book. Somebody throws in a card with some Chinese text, and you are expected to answer, also in Chinese. But you don't know Chinese!<sup class="footnote-ref"><a href="#fn4" id="fnref4">[4]</a></sup> Luckily, the book is here to help you: just look for the rules that apply to the symbols you got, and follow them to get some equally enigmatic Chinese symbols as a response.<sup class="footnote-ref"><a href="#fn5" id="fnref5">[5]</a></sup></p>
<p>Sounds familiar? It is more or less what chat bots do. They have no idea what each symbol is, or what the whole sentence means. They don't even split sentences in words like we do!<sup class="footnote-ref"><a href="#fn1" id="fnref1:1">[1:1]</a></sup> They just follow rules. With this example Searl was arguing that artificial intelligence models are incapable of understanding.</p>
<p>In other words, chat bots just spill <em>bullshit.</em> This is different from lying, because that would require understanding. Hallucinations are one of the biggest problems that large language models have, because they are not designed around the concepts of <em>truth</em> or <em>knowledge.</em> They are stochastic parrots, amazing bulshitters, professional Consequences players.</p>
<h2 id="empty-words%3A-plato%2C-aristotle" tabindex="-1">Empty words: Plato, Aristotle <a class="heading-link" href="#empty-words%3A-plato%2C-aristotle">§</a></h2>
<p>There is a rich European philosophical tradition of despising bullshit and nonsense, beginning with Socrates and Plato:</p>
<figure class="quote">
<blockquote cite="https://www.perseus.tufts.edu/hopper/text?doc=urn:cts:greekLit:tlg0059.tlg023.perseus-eng1:459b">
<p>Socrates: So he who does not know will be more convincing to those who do not know than he who knows, supposing the orator to be more convincing than the doctor. Is that, or something else, the consequence?</p>
</blockquote>
<figcaption class="quote-citation"><em>Gorgias</em> by Plato (p. 301, <a href="https://www.perseus.tufts.edu/hopper/text?doc=urn:cts:greekLit:tlg0059.tlg023.perseus-eng1:459b" class="link-external">459b</a> in <em>Plato in Twelve Volumes</em><sup class="footnote-ref"><a href="#fn6" id="fnref6">[6]</a></sup>).</figcaption>
</figure>
<p>And he adds about anything that is flattery and deceitful:</p>
<figure class="quote">
<blockquote cite="https://www.perseus.tufts.edu/hopper/text?doc=urn:cts:greekLit:tlg0059.tlg023.perseus-eng1:464e">
<p>Socrates: Flattery, however, is what I call it, and I say that this sort of thing is a disgrace [...] because it aims at the pleasant and ignores the best; and I say it is not an art, but a habitude, since it has no account to give of the real nature of the things it applies, and so cannot tell the cause of any of them. I refuse to give the name of art to anything that is irrational.</p>
</blockquote>
<figcaption class="quote-citation"><em>Gorgias</em> by Plato (p. 321, <a href="https://www.perseus.tufts.edu/hopper/text?doc=urn:cts:greekLit:tlg0059.tlg023.perseus-eng1:464e" class="link-external">464e</a> in <em>Plato in Twelve Volumes</em><sup class="footnote-ref"><a href="#fn6" id="fnref6:1">[6:1]</a></sup>).</figcaption>
</figure>
<p>To <strong>Plato,</strong> the appearance of wisdom without actual knowledge is just a &quot;knack of flattery&quot;, pretending rather than being. And boy, would he and Socrates have hated chat bots! The ultimate rhetoricians, words completely disconnected from actual knowledge.</p>
<p><strong>Aristotle</strong> also considers <em>logos</em> -reasoning- the structure of an argument, while <em>pathos</em> and <em>ethos</em> add emotional persuasion and credibility. In this context, any speech without <em>logos</em>, made of chains of words connected by probability, would fall apart. And indeed it does, if you push a chat bot a little bit.</p>
<h2 id="chatter%3A-kierkegaard%2C-heidegger" tabindex="-1">Chatter: Kierkegaard, Heidegger <a class="heading-link" href="#chatter%3A-kierkegaard%2C-heidegger">§</a></h2>
<p>Such stream of words devoid of meaning -both in the sense of knowledge and intention- would certainly irritate certain no-nonsense philosophers. For <strong>Søren Kierkegaard,</strong> the amount of <em>snak</em> -chatter- was already unbearable in the XIX century:</p>
<figure class="quote">
<blockquote>
<p>What is it to chatter? It is the annulment of the passionate disjunction between being silent and speaking. Only the person who can remain essentially silent can speak essentially, can act essentially. Silence is inwardness. Chattering gets ahead of essential speaking, and giving utterance to reflection has a weakening effect on action by getting ahead of it.</p>
</blockquote>
<figcaption class="quote-citation"><em>Two Ages. A Literary Review</em> by S. Kierkegaard (p. 97 in <em>Kierkegaard's Writings</em><sup class="footnote-ref"><a href="#fn7" id="fnref7">[7]</a></sup>).</figcaption>
</figure>
<p>For Kierkegaard, silence, inwardness and passion are the essence of an authentic existence. Large language models, with their endless chatter of empty words and total lack of agency are the extreme opposite. There is no inner life, there is no room for action.</p>
<p>Chatter was also despised by <strong>Martin Heidegger.</strong> But we need to better understand his ideas first, which are notoriously slippery. He had a curious theory of how humans perceive the world. For instance, how a tree can give us shadow or how a cookie is sweet food to be eaten is the primordial relation we have with these objects, while their shape, color or size are derivative properties used by the rational mind. Not only that, but focusing on the theory will deprive us from this practical knowledge.</p>
<p>After all, why <em>learning about an object</em> when you can <em>experience an action</em>? Theory will give you second-hand knowledge, mediated by others, while practice will give you an unfiltered, authentic experience. This is what Heidegger considers <em>authentic,</em> concerned with the world.</p>
<p>Unfortunately, we live constantly distracted, fascinated by the world of <em>others,</em> by their second-hand <em>idle talk.</em> After all, we are social animals and it is not possible to experience everything directly, we have to learn a whole lot. But this knowledge, once it starts circulating, is dettached, untrustworthy, superficial -in a practical sense- and it may distract us from the primordial experience.</p>
<figure class="quote">
<blockquote>
<p>What is said-in-the-talk as such, spreads in wider circles and takes on an authoritative character. Things are so because one says so. Idle talk is constituted by just such gossiping and passing the word along -a process by which its initial lack of grounds to stand on becomes aggravated to complete groundlessness. And indeed this idle talk is not confined to vocal gossip, but even spreads to what we write, where it takes the form of 'scribbling'. In this latter case the gossip is not based so much upon hearsay. It feeds upon superficial reading. The average understanding of the reader will never be able to decide what has been drawn from primordial sources with a struggle and how much is just gossip.</p>
</blockquote>
<figcaption class="quote-citation"><em>Being and Time</em> by Martin Heidegger (p. 212<sup class="footnote-ref"><a href="#fn8" id="fnref8">[8]</a></sup>).</figcaption>
</figure>
<p>This is why, from the point of view of Heidegger, large language models would be machines of pure <em>idle talk.</em> Their third-hand words are detached from the world, completely unathentic. The more we use a chat bot, the more it prevents us from experiencing the world directly.</p>
<h2 id="notes" tabindex="-1">Notes <a class="heading-link" href="#notes">§</a></h2>
<hr class="footnotes-sep">
<section class="footnotes">
<ol class="footnotes-list">
<li id="fn1" class="footnote-item"><p>Large language models use tokens, which are symbols smaller than words, for <a href="https://en.wikipedia.org/wiki/Large_language_model#Tokenization" class="link-wikipedia">complicated reasons</a>. For instance, this is how ChatGPT 4 reads a sentence: &quot;A| spect|re| is| haunting| Europe| —| the| spect|re| of| commun|ism|.&quot;. <a href="https://platform.openai.com/tokenizer" class="link-external">Try it!</a>. <a href="#fnref1" class="footnote-backref">↩︎</a> <a href="#fnref1:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn2" class="footnote-item"><p>Predicting the right word is more complex than just having a <a href="https://en.wikipedia.org/wiki/Markov_chain" class="link-wikipedia">Markov chain</a> with the probabilities between all tokens, for instance it required solving the <a href="https://en.wikipedia.org/wiki/Attention_Is_All_You_Need" class="link-wikipedia">attention</a> problem. <a href="#fnref2" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn3" class="footnote-item"><p>Well, this is how OpenAI calls the GPT API. They are act actually called <em>autoregressive large language models</em>. <a href="#fnref3" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn4" class="footnote-item"><p>If you do, pretend the thought experiment uses a different language that you don't know. <a href="#fnref4" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn5" class="footnote-item"><p>In many modern jobs it is easy to relate to the Chineese room operator, a feeling that David Graeber captures in <em>Bullshit Jobs: A Theory</em>. <a href="#fnref5" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn6" class="footnote-item"><p>Plato, <em>Gorgias</em> in <em>Plato in Twelve Volumes</em>, vol. III, trans. W. R. M. Lamb, Harvard University Press, 1967. <a href="#fnref6" class="footnote-backref">↩︎</a> <a href="#fnref6:1" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn7" class="footnote-item"><p>Søren Kierkegaard, <em>Two Ages. The Age Of Revolution And The Present Age. A Literary Review</em> in <em>Kierkegaard's Writings</em>, vol. XIV, p. 97, SV VIII 91, trans. Howard V. Hong and Edna H. Hong, Princeton University Press, 2009. <a href="#fnref7" class="footnote-backref">↩︎</a></p>
</li>
<li id="fn8" class="footnote-item"><p>Martin Heidegger, <em>Being and Time</em>, p. 212, SZ 169, trans. John Macquarrie and Edward Robinson, Blackwell, 1962. <a href="#fnref8" class="footnote-backref">↩︎</a></p>
</li>
</ol>
</section>



        </div>
        <div class="clear"></div>

    </article>
    <!-- <div class="brutalist_themes__related_posts">

        <h4 class="widget-title">You might also like</h4>

        <ul>
            <li itemscope="" itemtype="http://schema.org/blogPost">
                <a class="title" href="https://lamaquinadeturing.su/2020/05/01/cibernetica-al-servicio-del-comunismo/"
                    title="Cibernética – al servicio del comunismo">
                    <img src="index_fitxers/cybernetics-150x150.png"
                        class="attachment-thumbnail size-thumbnail wp-post-image"
                        alt="Cibernética – al servicio del comunismo" itemprop="image"
                        itemtype="http://schema.org/thumbnailUrl" width="150" height="150">
                    <h5 itemscope="" itemtype="http://schema.org/headline">Cibernética – al servicio del
                        comunismo</h5>
                </a>
            </li>
            <li itemscope="" itemtype="http://schema.org/blogPost">
                <a class="title"
                    href="https://lamaquinadeturing.su/2020/05/01/cuando-la-inteligencia-artificial-desplazo-la-cibernetica/"
                    title="Cuando la inteligencia artificial desplazó la cibernética">
                    <img src="index_fitxers/darthsmouth-1-150x150.jpg"
                        class="attachment-thumbnail size-thumbnail wp-post-image"
                        alt="Cuando la inteligencia artificial desplazó la cibernética" itemprop="image"
                        itemtype="http://schema.org/thumbnailUrl" width="150" height="150">
                    <h5 itemscope="" itemtype="http://schema.org/headline">Cuando la inteligencia artificial
                        desplazó la cibernética</h5>
                </a>
            </li>
        </ul>
        <div class="clear"></div>
    </div> -->
</div>

<div class="grid__item">

    <aside class="widget-area" itemscope="" itemtype="http://schema.org/WPSideBar">
    
    <div class="widget element__frame widget_recent_entries">
        <h3 class="widget-title">Recent Posts</h3>
        <ul>
            
                
            <li>
                <a href="/en/2025/08/philosophical-notes-on-llms-1-form-without-content/">Philosophical notes on LLMs 1: form without content</a>
                
            </li>
            
                
            <li>
                <a href="/ca/2025/06/pay-or-spain-cronica-del-mur-de-pagament-de-la-premsa-espanyola/">Pay or Spain: Crònica del mur de pagament de la premsa espanyola</a>
                
                    <a href="/ca/2025/06/pay-or-spain-cronica-del-mur-de-pagament-de-la-premsa-espanyola/" class="box-shadow lang-code">ca</a>
                    
                    
                
            </li>
            
                
            <li>
                <a href="/en/2025/02/this-post-has-been-generated-with-eleventy/">This post has been generated with Eleventy</a>
                
            </li>
            
                
            <li>
                <a href="/en/2024/10/git-rebase-doesnt-do-what-you-think/">Git rebase doesn't do what you think</a>
                
            </li>
            
                
            <li>
                <a href="/en/2024/09/simple-sabotage-field-manual/">Simple Sabotage Field Manual</a>
                
            </li>
            
        </ul>
    </div>
    <!-- <div id="archives-2" class="widget element__frame widget_archive">
        <h3 class="widget-title">Archives</h3>
        <ul>
            <li><a href="https://lamaquinadeturing.su/2021/08/">August 2021</a></li>
            <li><a href="https://lamaquinadeturing.su/2020/05/">May 2020</a></li>
            <li><a href="https://lamaquinadeturing.su/2020/04/">April 2020</a></li>
        </ul>
    </div> -->
    <div class="widget element__frame widget_categories">
        <h3 class="widget-title">Categories</h3>
        <ul>
            
            
            <li class="cat-item cat-item-2"><a href="/en/cybernetics/">Cybernetics</a></li>
            
            <li class="cat-item cat-item-2"><a href="/en/lzhenauka/">Lzhenauka</a></li>
            
            <li class="cat-item cat-item-2"><a href="/en/web/">Web</a></li>
            
        </ul>
    </div>
    <div class="clear"></div>
</aside>

</div>

        </section>
        

        <footer id="site-footer" itemscope="" itemtype="http://schema.org/WPFooter">
    <a href="https://creativecommons.org/licenses/by-sa/4.0/deed.ca">CC BY-SA</a> · La màquina de Turing ·
    <a href="/en/2024/08/this-website-does-not-use-cookies/">This website does not use cookies</a>
</footer>
    </div>

    <a class="element__frame scroll-top" href="#scroll-top" title="scroll top" style="display: none;">
        <p class="bar__title">Top</p>
    </a>
</body>
</html>